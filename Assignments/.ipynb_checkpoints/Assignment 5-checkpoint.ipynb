{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4170b5b",
   "metadata": {},
   "source": [
    "# Step 0 - import NN libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1926d058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6ec823",
   "metadata": {},
   "source": [
    "# Step 1 - Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eae739f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "030e51e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6b6afbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5865ee0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1880c945490>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "663e7453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e0be9",
   "metadata": {},
   "source": [
    "# Step 2 - Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaa041ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67f9a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37881343",
   "metadata": {},
   "source": [
    "# Step 3 - Divide dataset into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5ae225",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = x_train[55000:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e34647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = y_train[55000:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd38f9",
   "metadata": {},
   "source": [
    "# Step 4 - Build a simple dense network, use ExponentialLearningRate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3155a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca2e5a",
   "metadata": {},
   "source": [
    "# Step 5 - Use sigmoid, relu, and softmax as activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a65279bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4050d76d",
   "metadata": {},
   "source": [
    "# Step 6 - Plot loss as a function of learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6d9a49c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3444752.7500 - accuracy: 0.1162 - val_loss: 2.3034 - val_accuracy: 0.1060\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3025 - val_accuracy: 0.1060\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer= \"sgd\",\n",
    "metrics=[\"accuracy\"])\n",
    "\n",
    "fitted_model = model.fit(x_train,y_train,validation_data=(x_dev,y_dev),epochs=100,callbacks = [tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b179dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3444752.75,\n",
       "  2.301443338394165,\n",
       "  2.3013861179351807,\n",
       "  2.3014378547668457,\n",
       "  2.3013970851898193,\n",
       "  2.3013916015625,\n",
       "  2.30133056640625,\n",
       "  2.3013155460357666,\n",
       "  2.3012776374816895,\n",
       "  2.3012685775756836,\n",
       "  2.3012804985046387,\n",
       "  2.301257848739624,\n",
       "  2.3012595176696777,\n",
       "  2.3012218475341797,\n",
       "  2.301234722137451,\n",
       "  2.301203966140747,\n",
       "  2.3012099266052246,\n",
       "  2.301178216934204,\n",
       "  2.301201343536377,\n",
       "  2.301215648651123,\n",
       "  2.301196813583374,\n",
       "  2.301180839538574,\n",
       "  2.3011832237243652,\n",
       "  2.3011913299560547,\n",
       "  2.3011889457702637,\n",
       "  2.301182746887207,\n",
       "  2.3011837005615234,\n",
       "  2.3011844158172607,\n",
       "  2.3011772632598877,\n",
       "  2.3011655807495117,\n",
       "  2.3011820316314697,\n",
       "  2.3011715412139893,\n",
       "  2.301170825958252,\n",
       "  2.301170587539673,\n",
       "  2.3011717796325684,\n",
       "  2.301170587539673,\n",
       "  2.30116605758667,\n",
       "  2.301165819168091,\n",
       "  2.3011655807495117,\n",
       "  2.3011646270751953,\n",
       "  2.301164150238037,\n",
       "  2.301164150238037,\n",
       "  2.301164150238037,\n",
       "  2.3011603355407715,\n",
       "  2.3011622428894043,\n",
       "  2.3011646270751953,\n",
       "  2.3011631965637207,\n",
       "  2.3011605739593506,\n",
       "  2.3011600971221924,\n",
       "  2.301161050796509,\n",
       "  2.3011598587036133,\n",
       "  2.3011631965637207,\n",
       "  2.301161289215088,\n",
       "  2.301161527633667,\n",
       "  2.3011600971221924,\n",
       "  2.3011600971221924,\n",
       "  2.301159620285034,\n",
       "  2.301161527633667,\n",
       "  2.301161289215088,\n",
       "  2.3011598587036133,\n",
       "  2.301158905029297,\n",
       "  2.301159143447876,\n",
       "  2.3011586666107178,\n",
       "  2.3011600971221924,\n",
       "  2.301159620285034,\n",
       "  2.301159381866455,\n",
       "  2.3011574745178223,\n",
       "  2.301161527633667,\n",
       "  2.3011577129364014,\n",
       "  2.3011577129364014,\n",
       "  2.301157236099243,\n",
       "  2.301158905029297,\n",
       "  2.3011598587036133,\n",
       "  2.301161289215088,\n",
       "  2.301159143447876,\n",
       "  2.3011586666107178,\n",
       "  2.301158905029297,\n",
       "  2.301159381866455,\n",
       "  2.301158905029297,\n",
       "  2.3011584281921387,\n",
       "  2.3011586666107178,\n",
       "  2.3011574745178223,\n",
       "  2.3011586666107178,\n",
       "  2.3011586666107178,\n",
       "  2.301159620285034,\n",
       "  2.3011584281921387,\n",
       "  2.3011605739593506,\n",
       "  2.301154851913452,\n",
       "  2.3011603355407715,\n",
       "  2.3011603355407715,\n",
       "  2.3011574745178223,\n",
       "  2.3011598587036133,\n",
       "  2.301161050796509,\n",
       "  2.301161527633667,\n",
       "  2.3011581897735596,\n",
       "  2.301156997680664,\n",
       "  2.3011581897735596,\n",
       "  2.301159620285034,\n",
       "  2.301161766052246,\n",
       "  2.3011605739593506],\n",
       " 'accuracy': [0.11620000004768372,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581,\n",
       "  0.11236666887998581],\n",
       " 'val_loss': [2.303422212600708,\n",
       "  2.301710844039917,\n",
       "  2.3024702072143555,\n",
       "  2.301692485809326,\n",
       "  2.301699638366699,\n",
       "  2.3017685413360596,\n",
       "  2.301746368408203,\n",
       "  2.3017239570617676,\n",
       "  2.3017361164093018,\n",
       "  2.3017032146453857,\n",
       "  2.3017163276672363,\n",
       "  2.301790714263916,\n",
       "  2.3017568588256836,\n",
       "  2.3017168045043945,\n",
       "  2.3017513751983643,\n",
       "  2.3017821311950684,\n",
       "  2.301765203475952,\n",
       "  2.3017547130584717,\n",
       "  2.301705837249756,\n",
       "  2.301706552505493,\n",
       "  2.30171275138855,\n",
       "  2.3017091751098633,\n",
       "  2.301715135574341,\n",
       "  2.3017075061798096,\n",
       "  2.3017184734344482,\n",
       "  2.3017098903656006,\n",
       "  2.30172061920166,\n",
       "  2.3017146587371826,\n",
       "  2.3017194271087646,\n",
       "  2.301706314086914,\n",
       "  2.3017096519470215,\n",
       "  2.301708221435547,\n",
       "  2.3017077445983887,\n",
       "  2.3017067909240723,\n",
       "  2.301708459854126,\n",
       "  2.301710367202759,\n",
       "  2.301710605621338,\n",
       "  2.301710605621338,\n",
       "  2.301710367202759,\n",
       "  2.301711320877075,\n",
       "  2.301713705062866,\n",
       "  2.3017122745513916,\n",
       "  2.3017120361328125,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017120361328125,\n",
       "  2.3017120361328125,\n",
       "  2.3017117977142334,\n",
       "  2.3017125129699707,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.301711320877075,\n",
       "  2.301711320877075,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.301711320877075,\n",
       "  2.3017120361328125,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017120361328125,\n",
       "  2.3017120361328125,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.301711320877075,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017115592956543,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017122745513916,\n",
       "  2.3017115592956543,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017115592956543,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.301711320877075,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.3017117977142334,\n",
       "  2.301711320877075,\n",
       "  2.3017115592956543,\n",
       "  2.3017115592956543,\n",
       "  2.3017117977142334],\n",
       " 'val_accuracy': [0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246,\n",
       "  0.10599999874830246],\n",
       " 'lr': [0.008912509,\n",
       "  0.007943282,\n",
       "  0.0070794574,\n",
       "  0.006309573,\n",
       "  0.005623413,\n",
       "  0.005011872,\n",
       "  0.0044668354,\n",
       "  0.003981071,\n",
       "  0.0035481334,\n",
       "  0.0031622772,\n",
       "  0.0028183826,\n",
       "  0.0025118862,\n",
       "  0.002238721,\n",
       "  0.001995262,\n",
       "  0.0017782791,\n",
       "  0.0015848929,\n",
       "  0.0014125373,\n",
       "  0.0012589252,\n",
       "  0.0011220182,\n",
       "  0.0009999998,\n",
       "  0.0008912508,\n",
       "  0.0007943281,\n",
       "  0.00070794567,\n",
       "  0.00063095725,\n",
       "  0.00056234124,\n",
       "  0.00050118717,\n",
       "  0.00044668352,\n",
       "  0.0003981071,\n",
       "  0.00035481332,\n",
       "  0.0003162277,\n",
       "  0.00028183823,\n",
       "  0.00025118858,\n",
       "  0.00022387206,\n",
       "  0.00019952618,\n",
       "  0.0001778279,\n",
       "  0.00015848927,\n",
       "  0.00014125371,\n",
       "  0.0001258925,\n",
       "  0.000112201815,\n",
       "  9.9999976e-05,\n",
       "  8.9125075e-05,\n",
       "  7.943281e-05,\n",
       "  7.0794566e-05,\n",
       "  6.309572e-05,\n",
       "  5.6234123e-05,\n",
       "  5.0118713e-05,\n",
       "  4.466835e-05,\n",
       "  3.981071e-05,\n",
       "  3.5481335e-05,\n",
       "  3.1622774e-05,\n",
       "  2.8183827e-05,\n",
       "  2.5118863e-05,\n",
       "  2.238721e-05,\n",
       "  1.9952622e-05,\n",
       "  1.7782793e-05,\n",
       "  1.5848931e-05,\n",
       "  1.4125375e-05,\n",
       "  1.2589254e-05,\n",
       "  1.1220184e-05,\n",
       "  1e-05,\n",
       "  8.912509e-06,\n",
       "  7.943282e-06,\n",
       "  7.0794576e-06,\n",
       "  6.309573e-06,\n",
       "  5.623413e-06,\n",
       "  5.011872e-06,\n",
       "  4.4668354e-06,\n",
       "  3.981071e-06,\n",
       "  3.5481335e-06,\n",
       "  3.1622774e-06,\n",
       "  2.8183827e-06,\n",
       "  2.5118861e-06,\n",
       "  2.2387208e-06,\n",
       "  1.995262e-06,\n",
       "  1.7782792e-06,\n",
       "  1.584893e-06,\n",
       "  1.4125374e-06,\n",
       "  1.2589253e-06,\n",
       "  1.1220184e-06,\n",
       "  9.999999e-07,\n",
       "  8.912508e-07,\n",
       "  7.943281e-07,\n",
       "  7.079457e-07,\n",
       "  6.3095723e-07,\n",
       "  5.623412e-07,\n",
       "  5.011871e-07,\n",
       "  4.466835e-07,\n",
       "  3.981071e-07,\n",
       "  3.5481332e-07,\n",
       "  3.162277e-07,\n",
       "  2.8183823e-07,\n",
       "  2.511886e-07,\n",
       "  2.2387208e-07,\n",
       "  1.995262e-07,\n",
       "  1.7782791e-07,\n",
       "  1.584893e-07,\n",
       "  1.4125374e-07,\n",
       "  1.2589253e-07,\n",
       "  1.1220183e-07,\n",
       "  9.999999e-08]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_model.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitted_model.history['lr'], fitted_model.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd37ff",
   "metadata": {},
   "source": [
    "# Step 7 - What is the value of lr when loss shoots up? Report answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47ac96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training may not converge or even diverge if the learning rate is excessive. Weight fluctuations might be so large that the optimizer overshoots the minimum and worsens the loss.\n",
    "# The approach is to train a network with a low learning rate and gradually raise the learning rate with each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a319385e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1123 - val_loss: 2.3025 - val_accuracy: 0.1060\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3019 - val_accuracy: 0.1060\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3020 - val_accuracy: 0.1060\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3019 - val_accuracy: 0.1060\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer= \"sgd\",\n",
    "metrics=[\"accuracy\"])\n",
    "#early stopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "restore_best_weights=True)\n",
    "\n",
    "fitted_model_1 = model.fit(x_train,y_train,validation_data=(x_dev,y_dev),epochs=100,callbacks = [tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn),early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb8b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model_1.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f71587",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitted_model_1.history['lr'], fitted_model.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57747232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3021 - val_accuracy: 0.1060\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3023 - val_accuracy: 0.1060\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3021 - val_accuracy: 0.1060\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3019 - val_accuracy: 0.1060\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3019 - val_accuracy: 0.1060\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3018 - val_accuracy: 0.1060\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1060\n",
      "Epoch 27/100\n",
      " 687/1875 [=========>....................] - ETA: 1s - loss: 2.3012 - accuracy: 0.1119"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer= \"sgd\",\n",
    "metrics=[\"accuracy\"])\n",
    "#early stopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
    "restore_best_weights=True)\n",
    "\n",
    "#checkpoint\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\",\n",
    "save_best_only=True)\n",
    "\n",
    "fitted_model_2 = model.fit(x_train,y_train,validation_data=(x_dev,y_dev),epochs=100,callbacks = [tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn),early_stopping_cb,checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb1502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_model_2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d36bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fitted_model_2.history['lr'], fitted_model.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37771beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
